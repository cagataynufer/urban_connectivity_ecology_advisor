# -*- coding: utf-8 -*-
"""SecretaryAgent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aNtEXhA88FeHt_SKT424IZNTYoxzE7gx
"""

import os
import time
import shutil
import hashlib
import zipfile
from typing import List
from chromadb import PersistentClient
from huggingface_hub import hf_hub_download, HfApi
from tinydb import TinyDB

class SecretaryAgent:

    """
      A memory management agent responsible for:
      - Logging agent sessions with embeddings to ChromaDB
      - Managing both long-term and short-term (session) memory collections
      - Syncing memory states with Hugging Face (for reproducibility and persistence)
      - Retrieving embedded memories relevant to new queries
      - HAS NO LLM! Technically not an agent, but a historian

      Tracks semantic sessions using document embeddings, and uploads/downstreams logs to HF hub.
    """

    def __init__(
        self,
        repo_id: str,
        embedder,
        local_path: str = "/content/secretary_memory",
        collection_name: str = "memory_sessions",
        session_collection_name: str = "current_session_memory"
    ):
        """
          Args:
              repo_id (str): Hugging Face dataset ID where vector memory is synced.
              embedder (Any): An embedder with `.encode()` and `.embed_query()` methods.
              local_path (str): Path where Chroma vectorstore is stored locally.
              collection_name (str): Name for long-term memory collection.
              session_collection_name (str): Name for short-term session memory collection.
        """

        self.repo_id = repo_id
        self.local_path = local_path
        self.embedder = embedder
        self.collection_name = collection_name
        self.session_collection_name = session_collection_name
        self.tinydb_log_path = "./logs/memory.json"
        self.tinydb_zip_path = "./logs/tinydb_logs.zip"
        self.tinydb_repo_id = "cagatayn/agent_logs_tinydb"

        os.makedirs(local_path, exist_ok=True)

        if not os.path.exists(os.path.join(local_path, "chroma.sqlite3")):
            try:
                hf_hub_download(repo_id=repo_id, filename="chroma.sqlite3", local_dir=local_path)
                hf_hub_download(repo_id=repo_id, filename="index.pkl", local_dir=local_path)
                print("Long-term memory loaded from HuggingFace.")
            except Exception as e:
                print(f"No remote memory found ‚Äî starting fresh. ({e})")
        else:
            print("Local Chroma memory exists ‚Äî skipping remote download.")

        client = PersistentClient(path=local_path)
        self.collection = client.get_or_create_collection(name=collection_name)
        self.session_collection = client.get_or_create_collection(name=session_collection_name)

        print(f"[Secretary] PersistentClient ready at {self.local_path}")
        print(f"[Secretary] Initialized memory: local_path={self.local_path}, collection={self.collection_name}, session_collection={self.session_collection_name}")

        self.query_count = 0
        self.refresh_threshold = 20  # every 20 queries, refresh from LTM

    def add_session(self, session_id: str, query: str, final_essay: str, per_agent_context: dict):

        """
          Adds a session to memory by embedding the combined session content and storing
          in both short-term and long-term collections.

          Args:
              session_id (str): Unique session identifier.
              query (str): The original query from the user.
              final_essay (str): Final synthesized essay from all agents.
              per_agent_context (dict): Mapping of agent name to list of RAG chunks used.
        """

        print(f"[Secretary] Adding new session: {session_id} | Query length: {len(query)}, Essay length: {len(final_essay)}")
        doc_id = hashlib.md5(f"{session_id}_{query}".encode()).hexdigest()

        document = f"SESSION: {session_id}\nQUERY: {query}\n\n"
        for agent, context in per_agent_context.items():
            document += f"[AGENT: {agent}]\nRETRIEVED TEXTS:\n" + "\n\n".join(f"\"{t.strip()}\"" for t in context) + "\n\n"
        document += f"[FINAL ESSAY]\n{final_essay.strip()}\n\n</SESSION>"

        embedding = self.embedder.encode([document])[0]
        metadata = {
            "source": "secretary_memory",
            "session_id": session_id,
            "agent_names": ",".join(per_agent_context.keys()),  # ‚úÖ flatten
            "timestamp": time.time()
        }

        self.collection.add(documents=[document], embeddings=[embedding], metadatas=[metadata], ids=[doc_id])
        self.session_collection.add(documents=[document], embeddings=[embedding], metadatas=[metadata], ids=[doc_id])

        self.query_count += 1
        print(f"[Secretary] Session {session_id} added to memory. Query count: {self.query_count}")

        if self.query_count % self.refresh_threshold == 0:
            print(f"[Secretary] Query count {self.query_count} reached refresh threshold ({self.refresh_threshold})")
            self._refresh_memory()

        if self.query_count == 1:
            print("First session stored ‚Äî pushing memory to Hugging Face...")
            self.push_to_huggingface()
            self.push_tinydb_logs_to_huggingface()

    def _refresh_memory(self):

        """
        Periodically transfers all documents from the session collection to the long-term collection.
        """

        print(f"[Secretary] Refreshing long-term memory...")
        results = self.session_collection.peek()
        if results:
            for doc, emb, meta, doc_id in zip(
                results["documents"],
                results["embeddings"],
                results["metadatas"],
                results["ids"]
            ):
                self.collection.add(documents=[doc], embeddings=[emb], metadatas=[meta], ids=[doc_id])
            print(f"[Secretary] Added {len(results['documents']) if results else 0} documents from session memory to long-term memory.")

    def clear_session_memory(self):

        """
        Clears the short-term session memory collection.
        """

        print("Clearing short-term session memory")
        self.session_collection.delete(where={})
        print(f"[Secretary] üßπ Session memory cleared.")

    def retrieve_memory(self, query: str, top_k: int = 3, from_session: bool = False) -> List[str]:

        """
          Retrieves top-k relevant session memories based on query.

          Args:
              query (str): Query text to embed and compare.
              top_k (int): Number of results to return.
              from_session (bool): If True, retrieves from session memory only.

          Returns:
              List[str]: List of most similar memory documents.
        """

        print(f"[Secretary] Retrieving memory for query: '{query[:50]}...' | From session: {from_session}")
        vectorstore = self.session_collection if from_session else self.collection
        results = vectorstore.query(
            query_embeddings=[self.embedder.embed_query(query)],
            n_results=top_k
        )
        print(f"[Secretary] Retrieved {len(results.get('documents', [[]])[0])} results")
        return results.get("documents", [[]])[0]

    def push_to_huggingface(self):
        """
        Pushes the current local Chroma vectorstore to the Hugging Face Hub.
        Works with either the old  index_metadata.pickle  or the new  index_metadata.json .
        """

        api = HfApi()
        try:
            # ‚îÄ‚îÄ always upload the main SQLite ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            api.upload_file(
                path_or_fileobj=os.path.join(self.local_path, "chroma.sqlite3"),
                path_in_repo="chroma.sqlite3",
                repo_id=self.repo_id,
                repo_type="dataset"
            )

            # ‚îÄ‚îÄ find whichever metadata file exists ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            for fname in ("index_metadata.pickle", "index_metadata.json"):
                fpath = os.path.join(self.local_path, fname)
                if os.path.exists(fpath):
                    api.upload_file(
                        path_or_fileobj=fpath,
                        path_in_repo=fname,
                        repo_id=self.repo_id,
                        repo_type="dataset"
                    )
                    print(f"üì§ Uploaded {fname} to Hugging Face.")
                    break
            else:
                print("‚ö†Ô∏è  No Chroma metadata file found to upload.")

            print("[Secretary] üöÄ Memory push to Hugging Face completed.")

        except Exception as e:
            print(f"[Secretary] Failed to push vector memory: {e}")

    def push_tinydb_logs_to_huggingface(self):

        """
        Zips and pushes TinyDB log file to the Hugging Face Hub.
        """

        try:
            os.makedirs(os.path.dirname(self.tinydb_zip_path), exist_ok=True)
            with zipfile.ZipFile(self.tinydb_zip_path, "w") as zipf:
                zipf.write(self.tinydb_log_path, arcname="memory.json")

            api = HfApi()
            api.upload_file(
                path_or_fileobj=self.tinydb_zip_path,
                path_in_repo="tinydb_logs.zip",
                repo_id=self.tinydb_repo_id,
                repo_type="dataset"
            )
            print("TinyDB logs pushed to Hugging Face dataset.")
            print("[Secretary] TinyDB logs push to Hugging Face completed.")
        except Exception as e:
            print(f"Failed to push logs: {e}")

    def pull_latest_tinydb_log(self):

        """
        Pulls and extracts the latest zipped TinyDB log file from Hugging Face,
        if it doesn't already exist locally.
        """

        if not os.path.exists(self.tinydb_log_path):
            try:
                archive_path = hf_hub_download(
                    repo_id=self.tinydb_repo_id,
                    filename="tinydb_logs.zip",
                    repo_type="dataset"
                )
                os.makedirs(os.path.dirname(self.tinydb_log_path), exist_ok=True)
                shutil.unpack_archive(archive_path, os.path.dirname(self.tinydb_log_path))
                print("Pulled existing TinyDB log from Hugging Face.")
                print("[Secretary] TinyDB log successfully pulled and extracted.")
            except Exception as e:
                print(f"No existing log found ‚Äî starting fresh. Details: {e}")
        else:
            print("Local TinyDB already exists ‚Äî skipping remote download.")
            print("[Secretary] Local TinyDB log already available. No pull needed.")