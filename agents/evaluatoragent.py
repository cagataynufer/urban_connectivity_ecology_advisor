# -*- coding: utf-8 -*-
"""EvaluatorAgent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13VUvp-24j9LSgLizq22-vzWjotIIZ2r-
"""

import os
import uuid
import time
import shutil
import numpy as np
from typing import List, Optional, Dict, Any
from collections import defaultdict
from langchain.vectorstores import Chroma
from langchain.llms import HuggingFacePipeline
from huggingface_hub import snapshot_download
from sklearn.metrics.pairwise import cosine_similarity
from tinydb import TinyDB

from core.semantic_utils import SemanticTruncator, TRUNCATION_PROFILES

class EvaluatorAgent:

    """
      A self-supervised evaluation agent that performs post-hoc factuality and logic checks
      on synthesized multi-agent outputs. Can independently retrieve, assess, and trigger rerouting.

      Capabilities:
      - Self-RAG to cross-check claims
      - Domain RAG access for redundancy verification
      - LLM-based critique prompting
      - Override feedback construction
      - Structured rerouting signals
    """

    loaded_vectorstores = {}

    def __init__(self, llm, vectorstore_repo: str, embedding_model: Any, retriever_k: int = 20,  agents=None, domain_prompt=None, vectorstores=None, db=None,  default_top_p: float = 0.85, tokenizer=None, truncation_profiles: dict = TRUNCATION_PROFILES):

        """
          Args:
              llm (DirectLLMWrapper): Inference-ready language model wrapper.
              vectorstore_repo (str): Hugging Face dataset repo with Chroma vectorstore.
              embedding_model (E5Embedder): Embedding function for semantic scoring.
              retriever_k (int): Number of documents to retrieve per query.
              agents (dict): Optional reference to domain agent instances.
              domain_prompt (str): Optional prompt for guiding LLM feedback tone.
              vectorstores (dict): Optional access to agent-specific vectorstores.
              db (TinyDB): Optional memory logger.
              default_top_p (float): Default semantic nucleus threshold for document selection.
              tokenizer (Any): Tokenizer used for truncation/token-based limits.
              truncation_profiles (dict): Configs for how much to keep at each truncation stage.
        """

        self.llm = llm
        self.name = "evaluator"
        self.embedding_model = embedding_model
        self.agents = agents or {}
        self.domain_prompt = domain_prompt
        self.vectorstores = vectorstores or {}
        self.db = db
        self.vectorstore_repo = vectorstore_repo
        self.evaluator_vectorstore = self._load_vectorstore(vectorstore_repo, embedding_model)
        self.default_top_p = default_top_p
        self.retriever = self.evaluator_vectorstore.as_retriever(search_kwargs={"k": retriever_k})
        self.retriever_k = retriever_k
        self.tokenizer = tokenizer
        self.truncation_profiles = truncation_profiles

    def _load_vectorstore(self, repo_id: str, embedding_model: E5Embedder):

        """
        Loads the evaluator vector database from Huggingface, the database contains reports in comparison and critical contexts.
        """

        random_suffix = str(uuid.uuid4())[:4]
        persist_root = f"/content/vectorstores/{self.name}_{random_suffix}"


        # If a previous vectorstore folder exists, delete it to prevent lock errors
        if os.path.exists(persist_root):
            try:
                shutil.rmtree(persist_root)
                print(f"[{self.name}] Removed existing vectorstore folder to avoid Chroma lock issues.")
            except Exception as e:
                print(f"[{self.name}] Could not delete old vectorstore: {e}")

        # Downloading the vectorstore from Hugging Face
        local_dir = snapshot_download(repo_id=repo_id, repo_type="dataset", local_dir=persist_root)

        # Locating Chroma's sqlite3 backend
        for root, dirs, files in os.walk(local_dir):
            if "chroma.sqlite3" in files:
                persist_path = root
                break
        else:
            raise FileNotFoundError(f"[{self.name}] No chroma.sqlite3 found under {local_dir}")

        print(f"[{self.name}] Loaded vectorstore from: {persist_path}")
        collection_name = {
            "evaluator": "evaluator_agent"
        }.get(self.name)

        vectorstore = Chroma(
            persist_directory=persist_path,
            collection_name=collection_name,
            embedding_function=embedding_model
        )
        self.loaded_vectorstores[persist_root] = vectorstore
        return vectorstore

    def retrieve_context(
        self,
        query: str,
        top_p: Optional[float] = None
    ) -> Tuple[List[str], float, bool, List[float]]:

        top_p = top_p or self.default_top_p

        # ── Fast path: ask the store for docs *with* cosine-distance scores ──
        try:
            raw = self.evaluator_vectorstore.similarity_search_with_score(
                query, k=self.retriever_k
            )
            if not raw:
                return [], 0.0, False, []
            docs, dists = zip(*raw)                       # distance ∈ [0,2] for cosine
            sims = 1.0 - np.clip(dists, 0.0, 1.0)        # convert to similarity [0,1]

        # ── Fallback: old path (re-embed locally) ────────────────────────────
        except Exception:
            docs = self.retriever.get_relevant_documents(query)
            if not docs:
                return [], 0.0, False, []
            query_vec = self.evaluator_vectorstore._embedding_function.embed_query(query)
            texts     = [doc.page_content for doc in docs]
            doc_vecs  = self.evaluator_vectorstore._embedding_function.embed_documents(texts)
            sims      = cosine_similarity([query_vec], doc_vecs)[0]

        # ── Build display texts ──────────────────────────────────────────────
        texts = [
            f"[SOURCE: {doc.metadata.get('source', 'unknown')}] | ROW: "
            f"{doc.metadata.get('row', 'N/A')} {doc.page_content.strip()}"
            for doc in docs
        ]

        # ── Softmax nucleus filter (same as before) ──────────────────────────
        exp_sims = np.exp(sims - np.max(sims))
        probs    = exp_sims / np.sum(exp_sims)

        paired = list(zip(texts, sims, probs))
        paired.sort(key=lambda x: x[1], reverse=True)

        sharp_drop = False
        if len(paired) > 2:
            sharp_drop = paired[0][1] - paired[2][1] > 0.25

        selected, cumulative_p = [], 0.0
        for txt, sim, p in paired:
            selected.append(txt)
            cumulative_p += p
            if cumulative_p >= top_p:
                break

        avg_score = np.mean([sim for _, sim, _ in paired[:len(selected)]])
        sel_sims  = sims[: len(selected)]

        return selected, float(avg_score), sharp_drop, sel_sims


    def evaluate_factuality(self, essay: str, original_query: str, session_id: str = "default", agent_rags: dict = None):

        """
        Evaluates the factual correctness of a multi-agent essay by:
        - Performing self-RAG on the Evaluator’s own knowledge base.
        - Collecting RAG chunks from all domain agents' vectorstores.
        - Asking the LLM to detect and attribute any factual inaccuracies.

        Args:
            essay (str): The generated essay to evaluate.
            original_query (str): The user's original question.
            session_id (str): ID for session-scoped logging.
            agent_rags (dict): Optional agent-specific retrievers to pull external evidence.

        Returns:
            Dict containing:
                - trigger (bool): Whether a factual issue was detected.
                - agent (str): Name of the agent at fault (if any).
                - feedback (str): Textual override instruction for rerouting.
                - reason (str): Always "factual" if triggered.
                - raw_llm_output (str): The full response from the evaluator LLM.
        """

        agent_rags = agent_rags or {}

        # Using evaluator's OWN RAG context
        print(f"[Evaluator] Starting factuality evaluation with essay length: {len(essay)} characters")
        evaluator_rag, _, _, _ = self.retrieve_context(essay)
        print(f"[Evaluator] Retrieved {len(evaluator_rag)} context documents for evaluator self-RAG")
        profile_ret = self.truncation_profiles["retrieval_rag"]
        joined_eval_context = SemanticTruncator.truncate(
                                  blocks=evaluator_rag,
                                  embedder=self.embedding_model,
                                  tokenizer=self.tokenizer,
                                  max_tokens=profile_ret["max_tokens"],
                                  sim_threshold=profile_ret["sim_threshold"],
                                  reference_text=essay,
                                  mode="query")


        # Gathering all external domain agent RAG chunks
        supporting_evidence = []
        for domain, store in agent_rags.items():
            try:
                query_vec = self.embedding_model.embed_query(essay)

                # ── choose the right retrieval method ──────────────────────
                if hasattr(store, "similarity_search_by_vector"):          # raw Chroma, FAISS, etc.
                    results = store.similarity_search_by_vector(
                        query_vec, k=self.retriever_k
                    )
                else:                                                      # LangChain retriever wrapper
                    results = store.similarity_search(
                        essay, k=self.retriever_k
                    )
                # ───────────────────────────────────────────────────────────
                texts = [
                    f"[SOURCE: {doc.metadata.get('source', 'unknown')}] | ROW: "
                    f"{doc.metadata.get('row', 'N/A')} {doc.page_content.strip()}"
                    for doc in results
                ]
                supporting_evidence.extend(texts)

            except Exception as e:
                print(f"[Evaluator] Failed to retrieve from {domain}: {e}")


        joined_evidence = SemanticTruncator.truncate(
                                  blocks=supporting_evidence,
                                  embedder=self.embedding_model,
                                  tokenizer=self.tokenizer,
                                  max_tokens=profile_ret["max_tokens"],
                                  sim_threshold=profile_ret["sim_threshold"],
                                  mode="self")

        print(f"[Evaluator] Gathered {len(supporting_evidence)} external RAG chunks from domain agents")
        prompt = f"""
    {self.domain_prompt.strip() if self.domain_prompt else ""}

    Evaluator's own context, this is your own RAG database for critique:
    {joined_eval_context}

    Retrieved documents used to create the essay, from all domain agents:
    {joined_evidence}

    Check for:
    - Factual inaccuracies
    - Domain overlap
    - Misuse or misinterpretation of source material

    Original query:
    {original_query}

    Essay:
    \"\"\"\n{essay.strip()}\n\"\"\"

    If OK, respond:
    OK

    If problematic, respond in the following format:

    REWRITE
    AGENT: [Name of the agent responsible for the factual issue]
    OVERRIDE_TEXT: [Precise instruction on what the agent should revise or rewrite]
    SUGGESTED_FIX: [Brief reason why this change is needed, such as the precise false information used.]

    Note: You are supposed to identify the responsible agent yourself, based on the issue.
    """

        print(f"[Evaluator] Invoking LLM for factuality check. Prompt length: {len(prompt)} characters")
        print(f"[Evaluator] Prompt preview:\n{prompt[:500]}")
        result = self.llm.invoke(prompt).strip()
        print(f"[Evaluator] Factuality evaluation completed. Result starts with: {result[:30]}")
        if self.db:
            self.db.insert({
                "session_id": session_id,
                "type": "evaluation_factuality",
                "result": result,
                "essay": essay,
                "retrieved_docs_truncated": supporting_evidence,
                "retrieval_token_estimate": sum(len(doc.split()) for doc in supporting_evidence),
                "timestamp": time.time()
            })

        if result.startswith("REWRITE"):
            lines = result.splitlines()
            return {
                "trigger": True,
                "agent": next((l.replace("AGENT:", "").strip() for l in lines if l.startswith("AGENT:")), "unknown"),
                "problem_chunk": "[Full essay]",
                "feedback": next((l.replace("OVERRIDE_TEXT:", "").strip() for l in lines if l.startswith("OVERRIDE_TEXT:")), ""),
                "reason": "factual",
                "raw_llm_output": result
            }

        return {"trigger": False, "raw_llm_output": result}


    def evaluate_logic(self, final_essay: str, original_query: str, session_id: str = "default"):

        """
          Evaluates logical coherence of the final multi-agent essay.
          Checks for structural consistency, clarity, domain balance, and logical soundness.

          Steps:
          - Truncates essay for internal relevance.
          - Uses evaluator's own RAG context.
          - Pulls key chunks from agent vectorstores.
          - Prompts LLM to detect logic flaws and recommend precise agent rewrites.

          Args:
              final_essay (str): Essay to evaluate.
              original_query (str): Original user prompt.
              session_id (str): ID for logging and reroute trace.

          Returns:
              Dict:
                  - trigger (bool): Whether a logical issue was found.
                  - agent (str): Who should revise.
                  - feedback (str): Revision instruction.
                  - reason (str): Always "logic".
                  - raw_llm_output (str): Full LLM output string.
        """

        print(f"[Evaluator] Starting logic evaluation with essay length: {len(final_essay)} characters")
        all_evidence = []
        profile_prior = self.truncation_profiles["prior_context"]
        profile_ret = self.truncation_profiles["retrieval_rag"]
        for domain, retriever in self.vectorstores.items():
            try:
                evidence = retriever.similarity_search(original_query, k=3)
                all_evidence += [doc.page_content.strip() for doc in evidence]
            except:
                continue
        print(f"[Evaluator] Gathered {len(all_evidence)} external RAG evidences for logic check")
        essay_blocks = final_essay.split("\n\n")
        final_essay = SemanticTruncator.truncate(
                                  blocks=essay_blocks,
                                  embedder=self.embedding_model,
                                  tokenizer=self.tokenizer,
                                  max_tokens=profile_prior["max_tokens"],
                                  sim_threshold=profile_prior["sim_threshold"],
                                  mode="self")

        evaluator_rag, _, _, _ = self.retrieve_context(original_query)
        print(f"[Evaluator] Retrieved {len(evaluator_rag)} context documents for evaluator self-RAG (logic)")
        evaluator_context = "\n".join(evaluator_rag)
        joined_all_evidence = SemanticTruncator.truncate(
                                  blocks=all_evidence,
                                  embedder=self.embedding_model,
                                  tokenizer=self.tokenizer,
                                  max_tokens=profile_ret["max_tokens"],
                                  sim_threshold=profile_ret["sim_threshold"],
                                  reference_text=final_essay,
                                  mode="query")

        prompt = f"""
{self.domain_prompt.strip() if self.domain_prompt else ""}

Evaluator's own context, this is your own RAG database for critique:
{evaluator_context}

Original query:
{original_query}

Retrieved documents used to create the essay, from all domain agents (ecology & land use agent, urban connectivity & resilience agent, and community & policies agent):
{joined_all_evidence}

Evaluate the following essay for:
- Logical coherence
- Consistency between sections
- Structured reasoning
- Fair domain representation

Essay:
\"\"\"\n{final_essay.strip()}\n\"\"\"

If OK, respond:
OK

If problematic, respond in the following format:

REWRITE
AGENT: [Name of the agent responsible for the factual/logical issue]
OVERRIDE_TEXT: [Precise instruction on what the agent should revise or rewrite]
SUGGESTED_FIX: [Brief reason why this change is needed, such as contradiction, unsupported claim, etc.]

Note: You are supposed to identify the responsible agent yourself, based on the issue.
"""
        print(f"[Evaluator] Invoking LLM for logic check. Prompt length: {len(prompt)} characters")
        print(f"[Evaluator] Prompt preview:\n{prompt[:500]}")
        result = self.llm.invoke(prompt).strip()
        print(f"[Evaluator] Logic evaluation completed. Result starts with: {result[:30]}")

        if self.db:
            self.db.insert({
                "session_id": session_id,
                "type": "evaluation_logic",
                "result": result,
                "essay_used_truncated": final_essay,
                "token_estimate": len(final_essay.split()),
                "timestamp": time.time()
            })


        if result.startswith("REWRITE"):
            lines = result.splitlines()
            return {
                "trigger": True,
                "agent": next((l.replace("AGENT:", "").strip() for l in lines if l.startswith("AGENT:")), "unknown"),
                "problem_chunk": "[Full essay]",
                "feedback": next((l.replace("OVERRIDE_TEXT:", "").strip() for l in lines if l.startswith("OVERRIDE_TEXT:")), ""),
                "reason": "logic",
                "raw_llm_output": result
            }

        return {"trigger": False, "raw_llm_output": result}

    def evaluate(self, essay: str, agent_outputs: dict, original_query: str, session_id: str = "default", agent_rags: dict = None) -> dict:

        """
          Combines both factuality and logic evaluations.
          Triggers rerouting if either evaluation identifies an issue,
          and provides override feedback to the responsible agent.

          Args:
              essay (str): Final generated essay to be judged.
              agent_outputs (dict): Raw responses from each agent (used for agent targeting).
              original_query (str): The user's original input.
              session_id (str): Logging session identifier.
              agent_rags (dict): Optional dict of agent retrievers to fetch external support.

          Returns:
              Dict containing:
                  - reroute_triggered (bool): Whether a reroute is needed.
                  - logic_check_passed (bool)
                  - source_check_passed (bool)
                  - reroute_start (optional): Agent to reroute from.
                  - reason (optional): "logic" or "factual"
                  - override_text (optional): Instruction for rerouted agent
                  - problem_chunk (optional): Placeholder text for the full essay chunk
                  - final_logic_result (str): Raw LLM output from logic check
                  - final_factual_result (str): Raw LLM output from factual check
        """

        print(f"[Evaluator] Starting full evaluation pipeline...")
        logic_result = self.evaluate_logic(essay, original_query, session_id)
        factual_result = self.evaluate_factuality(essay, original_query, session_id, agent_rags)
        print(f"[Evaluator] Logic trigger: {logic_result['trigger']}, Factual trigger: {factual_result['trigger']}")
        reroute_result = None
        if logic_result["trigger"]:
            reroute_result = logic_result
        elif factual_result["trigger"]:
            reroute_result = factual_result

        if reroute_result:
            print(f"[Evaluator] Reroute triggered by {reroute_result['reason']} from agent {reroute_result['agent']}")
        else:
            print(f"[Evaluator] No reroute needed. Essay passed checks.")

        result = {
            "reroute_triggered": reroute_result is not None,
            "logic_check_passed": not logic_result["trigger"],
            "source_check_passed": not factual_result["trigger"]
        }

        if reroute_result:
            result.update({
                "reroute_start": reroute_result["agent"],
                "reason": reroute_result["reason"],
                "problem_chunk": reroute_result["problem_chunk"],
                "override_text": reroute_result["feedback"]
            })

        if self.db:
            self.db.insert({
                "session_id": session_id,
                "type": "evaluation_combined_result",
                "triggered": reroute_result is not None,
                "logic_passed": not logic_result["trigger"],
                "factual_passed": not factual_result["trigger"],
                "agent": reroute_result["agent"] if reroute_result else None,
                "reason": reroute_result["reason"] if reroute_result else None,
                "override_text": reroute_result["feedback"] if reroute_result else None,
                "timestamp": time.time()
            })

        result["final_logic_result"] = logic_result.get("raw_llm_output", "")
        result["final_factual_result"] = factual_result.get("raw_llm_output", "")
        return result