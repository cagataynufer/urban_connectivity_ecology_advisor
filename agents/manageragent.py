# -*- coding: utf-8 -*-
"""ManagerAgent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SKYIen7NzbucV8_dqTMHndkVi5KTSfG7
"""

import os
import time
import uuid
import numpy as np
from typing import List, Optional, Dict, Any
from collections import defaultdict
from tinydb import TinyDB
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from langchain.llms import HuggingFacePipeline

from core.semantic_utils import SemanticTruncator, TRUNCATION_PROFILES

from typing import List, Optional, Dict, Tuple, Any
class ManagerAgent:

    """
        A centralized orchestration agent that manages and coordinates multiple domain agents,
        handles retry logic and re-routing for low-confidence outputs, synthesizes their responses
        into a final essay, and tracks session memory through TinyDB.

        Responsibilities:
        - Vector-based agent ordering
        - Domain agent execution with RAG
        - Semantic truncation of output chains
        - Confidence-based rerouting
        - Final essay synthesis and contribution heatmap logging
    """

    def __init__(self, agents: Dict[str, object], llm: HuggingFacePipeline, evaluator: object, embedding_model: Any, db=None, prompt_template: str = "",session_id: Optional[str] = None,tokenizer=None, truncation_profiles: dict = TRUNCATION_PROFILES):

        """
        Args:
            agents (Dict[str, DomainAgent]): Dictionary of domain-specific agent instances.
            llm (DirectLLMWrapper or HuggingFacePipeline): LLM inference wrapper.
            evaluator (EvaluatorAgent): Evaluator for quality control and rerouting.
            embedding_model (E5Embedder): Embedding model for all vector ops.
            db (TinyDB, optional): Storage for session memory. Auto-created if None.
            prompt_template (str): Prompt used to frame the final synthesis.
            session_id (str, optional): Unique session context ID.
            tokenizer (optional): Tokenizer used for length estimation and truncation.
            truncation_profiles (dict): Config profiles for truncation stages.
        """

        self.agents = agents
        self.prompt_template = prompt_template
        self.llm = llm
        self.evaluator = evaluator
        self.embedding_model = embedding_model
        self.session_id = session_id or str(uuid.uuid4())[:8]
        self.tokenizer = tokenizer
        self.truncation_profiles = truncation_profiles
        if db is not None:
            self.db = db
        else:
            memory_path = "./logs"
            os.makedirs(memory_path, exist_ok=True)
            self.db = TinyDB(f"{memory_path}/session_{self.session_id}.json")
        self.agent_retry_counts = defaultdict(int)


    def adjust_top_p_for_agent(self, agent, max_retriever_p=0.95, step=0.02):

        """
          Increases an agentâ€™s `retriever_p` value slightly to widen the range of retrieved content,
          useful when re-running an agent due to flagged or weak output.

          This implements a form of semantic nucleus expansion over retries.

          Args:
              agent (DomainAgent): The agent instance to adjust.
              max_retriever_p (float): Upper cap for how wide the retriever_p can grow.
              step (float): Increment applied per retry attempt.
        """

        name = agent.name
        self.agent_retry_counts[name] += 1
        retry_count = self.agent_retry_counts[name]

        new_p = min(0.85 + step * retry_count, max_retriever_p)
        print(f"[Manager] Adjusting {name}'s retriever_p to {new_p:.2f} (retry #{retry_count})")
        agent.retriever_p = new_p



    @staticmethod
    def compute_agent_heatmap(agent_outputs: dict, final_essay: str, db_path: str, session_id: str, similarity_threshold: float = 0.6):

        """
        Compute and log a heatmap of agent contributions to the final essay.

        Parameters:
        - agent_outputs (dict): {agent_name: List[str]} where each list contains retrieved chunks.
        - final_essay (str): The final combined essay text.
        - db_path (str): Path to the TinyDB JSON file.
        - session_id (str): Unique identifier for the current session.
        - similarity_threshold (float): Threshold above which a chunk is considered "used".
        """

        # Preparing vectorizer
        all_texts = [final_essay]
        for chunks in agent_outputs.values():
            all_texts.extend(chunks)

        vectorizer = TfidfVectorizer().fit(all_texts)
        final_vec = vectorizer.transform([final_essay])

        heatmap_stats = {}

        for agent, chunks in agent_outputs.items():
            if not chunks:
                heatmap_stats[agent] = {
                    "retrieved_chunks": 0,
                    "used_chunks": 0,
                    "avg_similarity": 0.0}
                continue

            chunk_vecs = vectorizer.transform(chunks)
            sims = cosine_similarity(chunk_vecs, final_vec).flatten()
            used_chunks = np.sum(sims > similarity_threshold)
            avg_sim = float(np.mean(sims))
            heatmap_stats[agent] = {
                            "retrieved_chunks": len(chunks),
                            "used_chunks": int(used_chunks),
                            "avg_similarity": round(avg_sim, 4)}

        # Saving to TinyDB
        os.makedirs(os.path.dirname(db_path), exist_ok=True)
        db = TinyDB(db_path)
        db.table("heatmap_stats").insert({
            "session_id": session_id,
            "heatmap": heatmap_stats})

        return heatmap_stats

    def run(self, query: str, session_id: str = "default"):

        """
          Executes the full multi-agent reasoning workflow:
          1. Orders domain agents by semantic similarity to query.
          2. Runs each agent sequentially with accumulated context.
          3. Applies semantic truncation on agent outputs.
          4. Synthesizes a final essay from agent responses and retrieved contexts.
          5. Logs memory and heatmap data to TinyDB.

          Args:
              query (str): User's input or task prompt.
              session_id (str): Unique ID used for memory logging.

          Returns:
              Dict:
                  - raw_outputs: Raw per-agent completions
                  - final_essay: Synthesized final essay string
                  - heatmap: Contribution stats from each agent
        """

        print(f"[Manager] Starting with query: {query}")
        self.agent_retry_counts = defaultdict(int)
        for agent in self.agents.values():
            agent.retriever_p = 0.85

        # Ordering agents by vector similarity with query
        agent_scores = []
        for agent in self.agents.values():
            query_vec = self.embedding_model.embed_query(query)
            similarity = float(cosine_similarity([query_vec], [agent.routing_vector])[0][0])
            agent_scores.append((agent, similarity))

        agent_scores.sort(key=lambda x: x[1], reverse=True)
        sorted_agents = [x[0] for x in agent_scores]

        print("[Manager] Agent execution order:", [agent.name for agent in sorted_agents])

        # Sequentially running agents
        context = ""
        outputs = {}
        agent_outputs = []
        agent_contexts = {}  # for the heatmap


        for agent in sorted_agents:
            print(f"[Manager] Running {agent.name} with context length: {len(context)}")

            output = agent.run(query=query, context=context)
            print(f"[Manager] {agent.name} completed. Response length: {len(output['response']) if isinstance(output, dict) else 'Unknown'}")
            outputs[agent.name] = output["response"]
            agent_outputs.append(f"[AGENT: {agent.name}]\n{output['response']}")

            retrievals = output.get("retrieved_context", [])
            agent_contexts[agent.name] = retrievals  # Store retrieved chunks for this agent
            if retrievals:
                agent_outputs.append(f"[RETRIEVED by {agent.name}]\n" + "\n".join(retrievals))

            context += "\n" + output["response"]  # Chain output for the next agent
            profile = self.truncation_profiles["prior_context"]
            if self.db:
                self.db.insert({
                    "session_id": session_id,
                    "type": "agent_output",
                    "agent": agent.name,
                    "input": query,
                    "output": output,
                    "context_used_truncated": SemanticTruncator.truncate(
                                  blocks=list(outputs.values()),
                                  embedder=self.embedding_model,
                                  tokenizer=self.tokenizer,
                                  max_tokens=profile["max_tokens"],
                                  sim_threshold=profile["sim_threshold"],
                                  mode="self"),
                    "context_token_count": len(context.split()),
                    "timestamp": time.time()})

        profile_agent = self.truncation_profiles["agent_outputs"]
        agent_output_blocks = [f"[AGENT: {name}]\n{out}" for name, out in outputs.items()]
        truncated_blocks = SemanticTruncator.truncate(
                                  blocks=agent_output_blocks,
                                  embedder=self.embedding_model,
                                  tokenizer=self.tokenizer,
                                  max_tokens=profile_agent["max_tokens"],
                                  sim_threshold=profile_agent["sim_threshold"],
                                  mode="self")



        # Using LLM to combine outputs into final essay
        formatting_prompt = self.prompt_template.strip() + "\n\n"
        formatting_prompt += f"Original user query:\n{query.strip()}\n\n"
        formatting_prompt += "Domain agent responses and their retrieved context:\n\n"
        formatting_prompt += truncated_blocks
        formatting_prompt += "\n\n Please synthesize a final essay from these insights:\n"
        if len(formatting_prompt) > 8000:
            print("Formatting prompt is too long. Length:", len(formatting_prompt))
            formatting_prompt = formatting_prompt[:15000]



        print("Prompt length (chars):", len(formatting_prompt))
        print("Truncated blocks preview:\n", truncated_blocks[:1000])

        print(f"[Manager] Final essay synthesis starting. Prompt size: {len(formatting_prompt)} characters")
        print(f"[Manager] Prompt preview:\n{formatting_prompt[:500]}")

        final_essay = self.llm.invoke(formatting_prompt)
        print("LLM finished generating final essay.")
        heatmap = self.compute_agent_heatmap(
                    agent_outputs=agent_contexts,
                    final_essay=final_essay,
                    db_path="./logs/memory.json",
                    session_id=session_id)
        self.agent_contexts = agent_contexts


        if self.db:
            self.db.insert({
                "session_id": session_id,
                "heatmap": heatmap,
                "type": "final_essay",
                "input": query,
                "output": final_essay,
                "timestamp": time.time()})


        return {
                    "raw_outputs": outputs,
                    "final_essay": final_essay,
                    "heatmap": heatmap}

    def run_from_agent(
        self,
        query: str,
        start_agent_name: str,
        override_text: str,
        problem_chunk: str,
        original_query: str,
        agent_outputs: dict,
        reason: str,
        session_id: str = "default"
    ) -> dict:

        """
          Reruns the multi-agent pipeline from a specific agent after a flagged issue.
          Agents before the flagged one keep their outputs; all others rerun with updated context.

          Args:
              query (str): The updated query after evaluation feedback.
              start_agent_name (str): Name of the agent that produced the problematic output.
              override_text (str): Evaluator's explanation or correction instructions.
              problem_chunk (str): The specific text chunk that triggered reroute.
              original_query (str): The original user query before rerouting.
              agent_outputs (Dict[str, str]): Previously generated outputs by agent.
              reason (str): Reason for reroute (e.g., factual error, hallucination).
              session_id (str): Memory session ID for tracking.

          Returns:
              Dict:
                  - raw_outputs: Updated agent outputs
                  - final_essay: Regenerated final synthesis
                  - heatmap: New contribution heatmap
        """

        print(f"[Manager] Rerouting from {start_agent_name} due to flagged content.")

        # Sort agents as usual
        query_vec = self.embedding_model.embed_query(query)
        agent_scores = [
            (agent, float(cosine_similarity([query_vec], [agent.routing_vector])[0][0]))
            for agent in self.agents.values()]

        agent_scores.sort(key=lambda x: x[1], reverse=True)
        sorted_agents = [x[0] for x in agent_scores]

        # Locating start point
        start_index = next((i for i, a in enumerate(sorted_agents) if a.name == start_agent_name), None)
        if start_index is None:
            raise ValueError(f"Agent '{start_agent_name}' not found.")

        print(f"[Manager] Re-running from index {start_index} onwards.")
        if self.db:
            self.db.insert({
                "session_id": session_id,
                "type": "reroute_trigger",
                "triggered_by": reason,
                "start_agent": start_agent_name,
                "problem_chunk": problem_chunk,
                "override_text": override_text,
                "original_query": original_query,
                "reroute_time": time.time()})

        context = ""
        outputs = {}
        agent_outputs_list = []
        agent_contexts = {}
        profile_prior = self.truncation_profiles["prior_context"]
        for i, agent in enumerate(sorted_agents):
            if i < start_index:
                outputs[agent.name] = agent_outputs.get(agent.name, "[PREVIOUS OUTPUT UNAVAILABLE]")
                context_blocks = list(outputs.values())
                context = SemanticTruncator.truncate(
                                  blocks=context_blocks,
                                  embedder=self.embedding_model,
                                  tokenizer=self.tokenizer,
                                  max_tokens=profile_prior["max_tokens"],
                                  sim_threshold=profile_prior["sim_threshold"],
                                  mode="self")

                continue

            print(f"[Manager] Running {agent.name} (rerouted)")

            if i == start_index:
                self.adjust_top_p_for_agent(agent)
                retry_prompt = f"""
                Your previous output contained issues flagged during evaluation.

                Type of issue: {reason.upper()}

                Problematic Output:
                """
                {problem_chunk}
                """

                Evaluator's Feedback:
                """
                {override_text.strip()}
                """

                What to do:
                - Focus ONLY on correcting the part above
                - Do NOT fabricate facts or logic
                - If unsure, say so
                - Use prior agent output below to align with previous context

                Original user query:
                {original_query}

                Your previous full output:
                {agent_outputs.get(start_agent_name, '[NO PRIOR OUTPUT FOUND]')}

                Essay so far (agent-by-agent):
                {'\n'.join(agent_outputs_list)}"""
                response = agent.run(
                    query=query,
                    context=context,
                    override_text=retry_prompt,
                    retry=True,
                    session_id=session_id
                )
                print(f"[Manager] {agent.name} rerun completed.")
                output_text = response["response"] if isinstance(response, dict) else response

            else:
                correction_note = f"An upstream agent response was corrected. Align your output accordingly:\n\n{override_text.strip()}"
                response = agent.run(
                    query=query,
                    context=context + "\n" + correction_note,
                    session_id=session_id
                )
                print(f"[Manager] {agent.name} rerun completed.")
                output_text = response["response"] if isinstance(response, dict) else response

            outputs[agent.name] = output_text
            retrievals = response.get("retrieved_context", [])
            agent_contexts[agent.name] = retrievals
            context += "\n" + output_text

            if self.db:
                self.db.insert({
                    "session_id": session_id,
                    "type": "agent_rerun_output",
                    "agent": agent.name,
                    "input": query,
                    "output": output_text,
                    "was_rerouted": i >= start_index,
                    "is_flagged_agent": i == start_index,
                    "reason": reason if i == start_index else None,
                    "context_used_truncated": SemanticTruncator.truncate(
                                  blocks=list(outputs.values()),
                                  embedder=self.embedding_model,
                                  tokenizer=self.tokenizer,
                                  max_tokens=profile_prior["max_tokens"],
                                  sim_threshold=profile_prior["sim_threshold"],
                                  mode="self"),
                    "context_token_count": len(context.split()),
                    "override_text": override_text if i == start_index else None,
                    "problem_chunk": problem_chunk if i == start_index else None,
                    "timestamp": time.time()})

        # Building structured blocks for the LLM prompt
        final_output_blocks = []
        for agent in sorted_agents:
            agent_response = outputs[agent.name]
            retrievals = agent_contexts.get(agent.name, [])
            block = f"[AGENT: {agent.name}]\n{agent_response}"
            if retrievals:
                block += "\n[RETRIEVED CONTEXT]\n" + "\n".join(retrievals)
            final_output_blocks.append({
                "agent": agent.name,
                "response": agent_response,
                "retrievals": retrievals,
                "full_block": block})

        # Separating out blocks for truncation
        protected_blocks = [b["full_block"] for b in final_output_blocks if b["agent"] in outputs]
        retrieval_blocks = ["\n".join(b["retrievals"]) for b in final_output_blocks]
        profile_ret = self.truncation_profiles["retrieval_rag"]
        truncated_retrievals = SemanticTruncator.truncate(
                                  blocks=retrieval_blocks,
                                  embedder=self.embedding_model,
                                  tokenizer=self.tokenizer,
                                  max_tokens=profile_ret["max_tokens"],
                                  sim_threshold=profile_ret["sim_threshold"],
                                  reference_text=query,
                                  mode="query")


        formatting_prompt = self.prompt_template.strip() + "\n\n"
        formatting_prompt += f"Original user query:\n{query.strip()}\n\n"
        formatting_prompt += "Domain agent responses and their retrieved context:\n\n"
        formatting_prompt += "\n\n".join(protected_blocks) + "\n\n"
        formatting_prompt += "Condensed retrieval evidence:\n\n" + truncated_retrievals + "\n\n"
        formatting_prompt += f"""
        Please synthesize a **new, corrected version** of the essay.

        A previous version was flagged for issues. Some agents have re-generated their outputs, and others have adapted to those corrections.

        Your job is to:
        - Integrate the new outputs faithfully
        - Ensure logical and factual consistency
        - Avoid repeating past mistakes
        - Respect domain distinctions
        - Preserve insights that were not affected by the reroute"""

        print(f"[Manager] Final essay synthesis starting. Prompt size: {len(formatting_prompt)} characters")
        print(f"[Manager] Prompt preview:\n{formatting_prompt[:500]}")

        # Final essay synthesis
        final_essay = self.llm.invoke(formatting_prompt)
        heatmap = self.compute_agent_heatmap(
                    agent_outputs=agent_contexts,
                    final_essay=final_essay,
                    db_path="./logs/memory.json",
                    session_id=session_id)
        self.agent_contexts = agent_contexts

        if self.db:
            self.db.insert({
                            "session_id": session_id,
                            "type": "final_essay_post_reroute",
                            "heatmap": heatmap,
                            "input": query,
                            "output": final_essay,
                            "reroute_triggered": True,
                            "trigger_reason": reason,
                            "trigger_agent": start_agent_name,
                            "chunk_fixed": problem_chunk,
                            "override_used": override_text,
                            "timestamp": time.time()})

        return {
            "raw_outputs": outputs,
            "final_essay": final_essay,
            "heatmap": heatmap}