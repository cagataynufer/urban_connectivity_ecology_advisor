# -*- coding: utf-8 -*-
"""DomainAgent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19UBkAWxfBsnPq6Y40rW-pt5fqgTCXS1T
"""

import os
import uuid
import time
import shutil
import numpy as np
from typing import List, Optional, Dict, Tuple, Any
from huggingface_hub import snapshot_download
from sklearn.metrics.pairwise import cosine_similarity
from langchain.vectorstores import Chroma
from langchain.llms import HuggingFacePipeline

from core.semantic_utils import SemanticTruncator, TRUNCATION_PROFILES

class DomainAgent:
    """
    Represents a single domain-specific agent in a multi-agent system.
    Each agent performs RAG using its domain vectorstore and optionally long/short-term memory.
    """

    loaded_vectorstores = {}  # For caching

    def __init__(
        self,
        name: str,
        prompt_template: str,
        vectorstore_repo: str,
        llm: HuggingFacePipeline,
        embedding_model: Any,
        retriever_k: int = 30,
        min_similarity: float = 0.3,
        default_top_p: float = 0.85,
        memory_vectorstore: Optional[Chroma] = None,
        session_memory_vectorstore: Optional[Chroma] = None,
        tokenizer=None,
        truncation_profiles: dict = TRUNCATION_PROFILES,
        extra_vectorstores: Optional[list] = None
    ):
        self.session_id = "default"
        self.name = name
        self.domain_prompt = prompt_template
        self.vectorstore_repo = vectorstore_repo
        self.llm = llm
        self.retriever_k = retriever_k
        self.min_similarity = min_similarity
        self.routing_vector = self._load_readme_description(vectorstore_repo, embedding_model)
        self.sharp_drop_threshold = 0.25
        self.default_top_p = default_top_p
        self.retriever_p = default_top_p
        self.embedding_model = embedding_model
        self.tokenizer = tokenizer
        self.truncation_profiles = truncation_profiles
        self.extra_vectorstores = extra_vectorstores or []

        self.vectorstore = self._load_vectorstore(vectorstore_repo, embedding_model)
        self.memory_vectorstore = memory_vectorstore
        self.session_memory_vectorstore = session_memory_vectorstore

        self.retriever = self.vectorstore.as_retriever(search_kwargs={"k": retriever_k})

    def _load_readme_description(self, repo_id: str, embedding_model) -> np.ndarray:
        try:
            local_dir = snapshot_download(repo_id=repo_id, repo_type="dataset")
            readme_path = os.path.join(local_dir, "README.md")

            if not os.path.exists(readme_path):
                raise FileNotFoundError("README.md not found in downloaded dataset")

            with open(readme_path, 'r', encoding='utf-8') as f:
                text = f.read()
                embedding = embedding_model.encode([text])[0]
                return embedding

        except Exception as e:
            print(f"[{self.name}] README.md not found or failed to load: {e}")
            dummy_dim = embedding_model.encode([""])[0].shape[0]
            return np.zeros(dummy_dim)

    def _load_vectorstore(self, repo_id: str, embedding_model) -> Chroma:
        random_suffix = str(uuid.uuid4())[:4]
        persist_root = f"/content/vectorstores/{self.name}_{random_suffix}"

        if os.path.exists(persist_root):
            try:
                shutil.rmtree(persist_root)
                print(f"[{self.name}] Removed existing vectorstore folder.")
            except Exception as e:
                print(f"[{self.name}] Could not delete old vectorstore: {e}")

        local_dir = snapshot_download(repo_id=repo_id, repo_type="dataset", local_dir=persist_root)

        for root, dirs, files in os.walk(local_dir):
            if "chroma.sqlite3" in files:
                persist_path = root
                break
        else:
            raise FileNotFoundError(f"[{self.name}] No chroma.sqlite3 found under {local_dir}")

        print(f"[{self.name}] Loaded vectorstore from: {persist_path}")

        collection_map = {
            "ecology": "ecology_land_use_agent",
            "urban": "urban_connectivity_agent",
            "community": "community_policies_agent"
        }

        collection_name = collection_map.get(self.name)
        if not collection_name:
            raise ValueError(f"No collection name found for agent: {self.name}")

        vectorstore = Chroma(
            persist_directory=persist_path,
            collection_name=collection_name,
            embedding_function=embedding_model
        )
        self.loaded_vectorstores[persist_root] = vectorstore
        return vectorstore

    def retrieve_context(self, query: str, top_p: Optional[float] = None) -> Tuple[List[str], float, bool, List[float]]:
        top_p = top_p or self.default_top_p

        main_pairs = self.vectorstore.similarity_search_with_score(query, k=self.retriever_k)
        docs, sims = zip(*[(d, 1.0 - dist) for d, dist in main_pairs]) if main_pairs else ([], [])

        for vs in self.extra_vectorstores:
            try:
                pairs = vs.similarity_search_with_score(query, k=self.retriever_k)
                d_ex, s_ex = zip(*[(d, 1.0 - dist) for d, dist in pairs])
            except AttributeError:
                d_ex = vs.as_retriever(search_kwargs={"k": self.retriever_k}).get_relevant_documents(query)
                q_vec = self.vectorstore._embedding_function.embed_query(query)
                s_ex = cosine_similarity(
                    [q_vec],
                    self.vectorstore._embedding_function.embed_documents([d.page_content for d in d_ex])
                )[0]
            docs += d_ex
            sims += s_ex

        if self.session_memory_vectorstore:
            m_docs = self.session_memory_vectorstore.similarity_search(query, k=self.retriever_k)
            if m_docs:
                q_vec = self.vectorstore._embedding_function.embed_query(query)
                sims_m = cosine_similarity(
                    [q_vec],
                    self.vectorstore._embedding_function.embed_documents([d.page_content for d in m_docs])
                )[0]
                docs += tuple(m_docs)
                sims += tuple(sims_m)

        if self.memory_vectorstore:
            ltm_pairs = self.memory_vectorstore.similarity_search_with_score(
                query=query,
                k=self.retriever_k,
                filter={"session_id": self.session_id}
            )
            l_docs, l_sims = zip(*[(d, 1.0 - dist) for d, dist in ltm_pairs]) if ltm_pairs else ([], [])
            docs += l_docs
            sims += l_sims

        texts = [
            f"[SOURCE: {d.metadata.get('source', 'unknown')}] | ROW: {d.metadata.get('row', 'N/A')} {d.page_content.strip()}"
            for d in docs
        ]
        print(f"[{self.name}] Retrieved {len(texts)} documents.")

        sims = np.array(sims)
        exp = np.exp(sims - np.max(sims))
        probs = exp / exp.sum()

        ranked = sorted(zip(texts, sims, probs), key=lambda x: x[1], reverse=True)

        top_sim = ranked[0][1]
        third_sim = ranked[2][1] if len(ranked) > 2 else ranked[-1][1]
        sharp_drop = (top_sim - third_sim) > 0.25

        selected, cumulative_p = [], 0.0
        for txt, sim, p in ranked:
            selected.append(txt)
            cumulative_p += p
            if cumulative_p >= top_p:
                break

        avg_score = float(np.mean([sim for _, sim, _ in ranked[:len(selected)]]))
        selected_sims = sims[:len(selected)]

        return selected, avg_score, sharp_drop, selected_sims

    def run(self, query: str, context: Optional[str] = None, override_text: Optional[str] = None, retry: bool = False, session_id: str = "default") -> Dict:
        self.session_id = session_id
        rag_context, avg_score, sharp_drop, sims = self.retrieve_context(query)
        low_confidence = avg_score < self.min_similarity or sharp_drop

        prompt_parts = [self.domain_prompt.strip()]

        if override_text:
            prompt_parts.append("Revision Request:\n" + override_text.strip())

        if context:
            profile = self.truncation_profiles["prior_context"]
            truncated_context = SemanticTruncator.truncate(
                blocks=context.split("\n") if isinstance(context, str) else context,
                embedder=self.embedding_model,
                tokenizer=self.tokenizer,
                max_tokens=profile["max_tokens"],
                sim_threshold=profile["sim_threshold"],
                reference_text=query,
                mode="self"
            )
            prompt_parts.append("Prior Agent Context:\n" + truncated_context.strip())

        prompt_parts.append(f"You may refer to the current session's history using session ID: {session_id}")

        profile_rag = self.truncation_profiles["retrieval_rag"]
        truncated_rag = SemanticTruncator.truncate(
            blocks=rag_context,
            embedder=self.embedding_model,
            tokenizer=self.tokenizer,
            max_tokens=profile_rag["max_tokens"],
            sim_threshold=profile_rag["sim_threshold"],
            reference_text=query,
            mode="query"
        )
        prompt_parts.append("Retrieved Documents:\n" + truncated_rag)

        final_prompt = "\n\n".join(prompt_parts)
        token_count = len(self.tokenizer.encode(final_prompt, add_special_tokens=False))

        if token_count > 7500:
            profile_prompt = self.truncation_profiles["final_prompt"]
            print(f"[{self.name}] Final prompt too long ({token_count} tokens). Applying truncation...")
            final_prompt = SemanticTruncator.truncate(
                blocks=prompt_parts,
                embedder=self.embedding_model,
                tokenizer=self.tokenizer,
                max_tokens=profile_prompt["max_tokens"],
                sim_threshold=profile_prompt["sim_threshold"],
                mode="self"
            )

        response = self.llm.invoke(final_prompt)

        return {
            "agent": self.name,
            "query": query,
            "mode": "retry" if retry else "default",
            "retry": retry,
            "low_confidence": low_confidence,
            "avg_score": avg_score,
            "context_used": context,
            "override_text": override_text,
            "response": response,
            "retrieved_context": rag_context,
            "timestamp": time.time()
        }