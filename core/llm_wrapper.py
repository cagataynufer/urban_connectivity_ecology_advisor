# -*- coding: utf-8 -*-
"""core_llm_wrapper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16KokDfu9e7fHYAR-HPGNrX7wtLZdRjt2
"""

import torch

class DirectLLMWrapper:
    """
    A lightweight, configurable wrapper around Hugging Face transformer models
    for generating responses from LLMs using manual decoding settings.

    Supports prompt slicing, device control, and reusable decoding configs.
    """

    def __init__(
        self,
        model,
        tokenizer,
        max_new_tokens=512,
        temperature=0.7,
        top_p=0.9,
        repetition_penalty=1.1,
        do_sample=True,
        return_full_text=False,
        pad_token_id=None,
        device="cuda"
    ):
        self.model = model
        self.tokenizer = tokenizer
        self.max_new_tokens = max_new_tokens
        self.temperature = temperature
        self.top_p = top_p
        self.repetition_penalty = repetition_penalty
        self.do_sample = do_sample
        self.return_full_text = return_full_text
        self.pad_token_id = pad_token_id or self.tokenizer.eos_token_id
        self.device = device

    def invoke(self, prompt: str) -> str:
        """
        Generates text continuation for a given prompt using defined decoding settings.

        Args:
            prompt (str): The input prompt for the model.

        Returns:
            str: The generated continuation (or full output if specified).
        """
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            padding=True,
            max_length=4096
        ).to(self.device)

        with torch.no_grad():
            outputs = self.model.generate(
                input_ids=inputs.input_ids,
                attention_mask=inputs.attention_mask,
                max_new_tokens=self.max_new_tokens,
                temperature=self.temperature,
                top_p=self.top_p,
                repetition_penalty=self.repetition_penalty,
                do_sample=self.do_sample,
                pad_token_id=self.pad_token_id
            )

        generated = outputs[0]

        if self.return_full_text:
            # If return_full_text=True, return full input + output
            return self.tokenizer.decode(generated, skip_special_tokens=True)
        else:
            # Otherwise, only return *new* generated tokens
            generated_text = self.tokenizer.decode(generated, skip_special_tokens=True)
            prompt_text = self.tokenizer.decode(inputs.input_ids[0], skip_special_tokens=True)
            if generated_text.startswith(prompt_text):
                return generated_text[len(prompt_text):].strip()
            else:
                return generated_text.strip()