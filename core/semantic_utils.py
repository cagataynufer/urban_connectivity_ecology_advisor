# -*- coding: utf-8 -*-
"""core_semantic_utils.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17ZpjWwUEM7FKAWB2yIhhhRrhwbn1qpg0
"""

import re
import numpy as np
import textwrap
from typing import List, Optional
from sklearn.metrics.pairwise import cosine_similarity

TRUNCATION_PROFILES = {
    "agent_outputs": {"max_tokens": 2500, "sim_threshold": 0.45},
    "prior_context": {"max_tokens": 2500, "sim_threshold": 0.3},
    "retrieval_rag": {"max_tokens": 3500, "sim_threshold": 0.25},
    "final_prompt": {"max_tokens": 8000, "sim_threshold": 0.25}
}

class SemanticTruncator:
    """Token-aware truncator that keeps only the most relevant (and non-duplicate) text."""

    @staticmethod
    def truncate(
        blocks: List[str],
        embedder,
        tokenizer=None,
        max_tokens: int = 8_000,
        reference_text: Optional[str] = None,
        sim_threshold: float = 0.25,
        max_block_chars: int = 1_500,
        mode: str = "query",              # 'query' or 'self'
        precomputed_sims: Optional[List[float]] = None
    ) -> str:

        def estimate_tokens(txt):        # naÃ¯ve fallback if no tokenizer
            return len(tokenizer.encode(txt, add_special_tokens=False)) if tokenizer else int(len(txt) / 4)

        def split_wrap(txt):
            return textwrap.wrap(txt, width=max_block_chars, break_long_words=False)

        # split + early dedupe
        split_blocks, sig_set = [], set()
        for blk in blocks:
            for piece in split_wrap(blk.strip()):
                sig = piece.lower().strip()
                if sig and sig not in sig_set:
                    split_blocks.append(piece)
                    sig_set.add(sig)

        if not split_blocks:
            return ""

        # get similarities
        if precomputed_sims is not None:
            if len(precomputed_sims) != len(split_blocks):
                raise ValueError("precomputed_sims length mismatch")
            sims = np.array(precomputed_sims)

        else:
            bl_embs = embedder.encode(split_blocks)
            if mode == "query":
                if reference_text is None:
                    raise ValueError("reference_text required for mode='query'")
                ref_vec = embedder.encode([reference_text])[0]
                sims = cosine_similarity([ref_vec], bl_embs)[0]
            elif mode == "self":
                sims = cosine_similarity(bl_embs, bl_embs).mean(axis=1)
            else:
                raise ValueError("mode must be 'query' or 'self'")

        # filter + sort
        scored = [(blk, sim) for blk, sim in zip(split_blocks, sims) if sim >= sim_threshold]
        scored.sort(key=lambda x: x[1], reverse=True)

        # hard dedupe after scoring
        seen, final_scored = set(), []
        for blk, sim in scored:
            sig = re.sub(r"\(reversed\)", "", blk.lower()).strip()[:120]
            if sig not in seen:
                final_scored.append((blk, sim))
                seen.add(sig)

        # greedy selection under token budget
        out, token_total = [], 0
        for blk, _ in final_scored:
            t = estimate_tokens(blk)
            if token_total + t > max_tokens:
                break
            out.append(blk)
            token_total += t

        return "\n\n".join(out)