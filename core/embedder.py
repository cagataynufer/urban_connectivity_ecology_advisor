# -*- coding: utf-8 -*-
"""core/embedder.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HC0JWqb_RmRBBNCdl9v399f8xgyQn6lh
"""

from transformers import AutoTokenizer, AutoModel
import torch
from typing import List

class E5Embedder:
    """
    Custom embedding wrapper for E5-style models like `intfloat/multilingual-e5-large`.
    This class supports query and document embedding with mean pooling.
    """

    def __init__(self, model_name="intfloat/multilingual-e5-large"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name)
        self.model.eval()

    def encode(self, texts, batch_size=32):
        """
        Encodes a list of texts into dense vector embeddings using mean pooling.
        Adds 'query: ' prefix automatically for E5 models.
        """
        all_embeddings = []

        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            if not batch:
                continue

            inputs = self.tokenizer(
                [f"query: {t}" for t in batch],
                padding=True,
                truncation=True,
                return_tensors="pt"
            )

            with torch.no_grad():
                outputs = self.model(**inputs)
                embeddings = outputs.last_hidden_state.mean(dim=1)

            all_embeddings.extend([e.cpu().numpy() for e in embeddings])

        return all_embeddings

    def embed_query(self, query: str):
        """
        Embeds a single query into a dense vector.
        """
        return self.encode([query])[0]

    def embed_documents(self, docs: List[str]):
        """
        Embeds a list of documents. Same as encode, but for clarity.
        """
        return self.encode(docs)